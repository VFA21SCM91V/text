{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF Text On Device Guide",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_as3tyDPAvzM"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CoWjX1EBXJX"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hQmWrtkBBQB"
      },
      "source": [
        "# Converting text ops to TF Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmGnheU8BPKN"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/guide/tflite\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/tflite.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/guide/tflite.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/guide/tflite.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz1hOEHPTF2n"
      },
      "source": [
        "## Overview\n",
        "\n",
        "There has been an increased emphasis on serving models on mobile devices with advances in mobile technology and the need for local models to protect privacy and decrease latency. We've taken many of our TensorFlow ops and provide them for use within the TensorFlow Lite environment as well. Performance and size has been the focus for these TF Lite compatible ops. As such, many of these ops have been rewritten from the ground up.\n",
        "\n",
        "The TensorFlow Lite compatible ops can be grouped into two categories. The first are normal TF Text ops that can be used from within TensorFlow or TF Lite. The second are ops written particularly for the mobile platform in mind and reside in the mobile subpackage of TF Text. Often, these have a modified API from the TensorFlow version with a more limited feature-set, or a fused combination of a common op sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZalFZQvTJf5",
        "outputId": "9e7ba769-9b2c-401f-d26a-353fcf3ced3f"
      },
      "source": [
        "!pip install -q -U tf-nightly\n",
        "!pip install -q -U tensorflow-text-nightly"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mdIyFfqTMjc"
      },
      "source": [
        "## Ops\n",
        "\n",
        "The following table lists the current ops provided by the TF Text package that can be used from within a TF Lite model. Also listed is the registration function required for inference.\n",
        "\n",
        "| Op | TF Lite Registration Function |\n",
        "| --- | --- |\n",
        "| FastWordpieceTokenizer.detokenize | AddFastWordpieceDetokenize |\n",
        "| FastWordpieceTokenizer.tokenize | AddFastWordpieceTokenize |\n",
        "| FastWordpieceTokenizer.tokenizeWithOffsets | AddFastWordpieceTokenize |\n",
        "| WhitespaceTokenizer.tokenize | AddWhitespaceTokenize |\n",
        "| WhitespaceTokenizer.tokenizeWithOffsets | AddWhitespaceTokenize |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6NAs1fcUwUn"
      },
      "source": [
        "## Model Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL-I0CyPTXnN"
      },
      "source": [
        "from absl import app\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "from tensorflow.lite.python import interpreter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj_bJ-xVTfU1"
      },
      "source": [
        "To show the conversion process and interpretation in Python, we create a very simple test model using the WhitespaceTokenizer. It should be noted that the output of a model cannot be RaggedTensor for TF Lite; however, you can return the components of the RaggedTensor. View this link to learn more about RaggedTensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqQjBcXqTf_0"
      },
      "source": [
        "class TokenizerModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.tokenizer = tf_text.WhitespaceTokenizer()\n",
        "\n",
        "  def call(self, input_tensor, **kwargs):\n",
        "    return self.tokenizer.tokenize(input_tensor).flat_values"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsPFI-55TiF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721b9c61-feab-4a17-e09f-c9f4775a1ed9"
      },
      "source": [
        "# Test input data.\n",
        "input_data = np.array([['Some minds are better kept apart']])\n",
        "\n",
        "# Define a Keras model.\n",
        "model = TokenizerModel()\n",
        "\n",
        "# Perform TF.Text inference.\n",
        "tf_result = model(tf.constant(input_data))\n",
        "print('TensorFlow result = ', tf_result)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKpFsvJGTlPq"
      },
      "source": [
        "\n",
        "## Convert the TF model to TF Lite\n",
        "\n",
        "Please review the [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert) documentation for a detailed guide for converting a model. When converting a TensorFlow model with TF Text ops to TF Lite, you need to\n",
        "indicate to the `TFLiteConverter` that there are custom ops using the\n",
        "`allow_custom_ops` attribute as in the example below. You can then convert as normal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hYWezs1Tndo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af5faf6-96a5-41ae-b876-e3934756cd7f"
      },
      "source": [
        "# Convert to TFLite.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "converter.allow_custom_ops = True\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxCdhrHATpSR"
      },
      "source": [
        "## Inference\n",
        "\n",
        "When reading a model by the TF Lite interpreter, we must indicate to it that we have custom ops, and must provide a registration method for those custom ops. The registration methods are listed above in the [Ops](#scrollTo=_mdIyFfqTMjc) section. Using the `tflite_registrar`, we provide the registration op for the `WhitespaceTokenizer` to the `InterpreterWithCustomOps`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kykFg2pXTriw"
      },
      "source": [
        "# Do TFLite inference.\n",
        "registration_fn = tf_text.tflite_registrar.AddWhitespaceTokenize\n",
        "interp = interpreter.InterpreterWithCustomOps(\n",
        "    model_content=tflite_model, custom_op_registerers=[registration_fn])\n",
        "input_details = interp.get_input_details()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNGPpHCCTxVX"
      },
      "source": [
        "The rest of the steps are common for TF Lite inference. TF Lite must know the input shape beforehand, so we set the input size before calling `allocate_tensors`. Next, we provide the input and then can invoke the interpreter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmSbfbgJTyKY"
      },
      "source": [
        "interp.resize_tensor_input(input_details[0]['index'], tf.shape(input_data))\n",
        "interp.allocate_tensors()\n",
        "interp.set_tensor(input_details[0]['index'], input_data)\n",
        "interp.invoke()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCKEWeldTzl0"
      },
      "source": [
        "Finally, we can see our TF Lite result is the same as the TensorFlow result from above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uks5ul4GT1Nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dec1cb4-ff60-4ec4-adef-36faf8482bc6"
      },
      "source": [
        "output_details = interp.get_output_details()\n",
        "tflite_result = interp.get_tensor(output_details[0]['index'])\n",
        "print('TFLite result = ', tflite_result)"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}